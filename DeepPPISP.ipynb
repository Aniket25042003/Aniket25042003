{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPP+ucw4e5TcfU+bckVF5S+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aniket25042003/Aniket25042003/blob/main/DeepPPISP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__init__.py (generator)"
      ],
      "metadata": {
        "id": "-bLzb4sEkS5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3OTpedpjq2E"
      },
      "outputs": [],
      "source": [
        "from .data_generator import dataSet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : Imports a module named \"dataset\" from the data_generator\" package or module.\n",
        "2. Why : Importing the \"dataSet\" module allows you to access the functions, classes, or variables defined within that module."
      ],
      "metadata": {
        "id": "RTrY4m8qleD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_generator.py"
      ],
      "metadata": {
        "id": "vUJ8RxJUkXwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- encoding:utf8 -*-\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import torch as t\n",
        "import numpy as np\n",
        "from torch.utils import data\n",
        "\n",
        "\n",
        "#my lib\n",
        "from utils.config import DefaultConfig\n",
        "\n",
        "\n",
        "\n",
        "# Creating PyTorch class to custom datasets\n",
        "class dataSet(data.Dataset):\n",
        "    # Intializing datasets\n",
        "    def __init__(self,window_size,sequences_file=None,pssm_file=None, dssp_file=None, label_file=None, protein_list_file=None):\n",
        "        super(dataSet,self).__init__()\n",
        "\n",
        "        # rb - read in binary\n",
        "\n",
        "        # List to store sequences\n",
        "        self.all_sequences = []\n",
        "        for seq_file in sequences_file:\n",
        "            with open(seq_file,\"rb\") as fp_seq: # Opening seq_file.pkl file\n",
        "               temp_seq  = pickle.load(fp_seq) # Unpickling seq_file.pkl file\n",
        "            self.all_sequences.extend(temp_seq) # Storing data into the list\n",
        "\n",
        "        # List to store PSSM\n",
        "        self.all_pssm = []\n",
        "        for pm_file in pssm_file:\n",
        "            with open(pm_file,\"rb\") as fp_pssm: # Opening pm_file.pkl file\n",
        "                temp_pssm = pickle.load(fp_pssm) # Unpickling pm_file.pkl file\n",
        "            self.all_pssm.extend(temp_pssm) # Storing data into the list\n",
        "\n",
        "        # List to store DSSP\n",
        "        self.all_dssp = []\n",
        "        for dp_file in dssp_file:\n",
        "            with open(dp_file,\"rb\") as fp_dssp: # Opening dp_file.pkl file\n",
        "                temp_dssp  = pickle.load(fp_dssp) # Unpickling dp_file.pkl file\n",
        "            self.all_dssp.extend(temp_dssp) # Storing data into the list\n",
        "\n",
        "        # List to store label\n",
        "        self.all_label = []\n",
        "        for lab_file in label_file:\n",
        "            with open(lab_file, \"rb\") as fp_label: # Opening lab_file.pkl file\n",
        "                temp_label = pickle.load(fp_label) # Unpickling lab_file.pkl file\n",
        "            self.all_label.extend(temp_label) # Storing data into the list\n",
        "\n",
        "        with open(protein_list_file, \"rb\") as list_label: # Opening protein_list_file.pkl file\n",
        "            self.protein_list = pickle.load(list_label) # Unpickling protein_list_file.pkl file\n",
        "\n",
        "\n",
        "\n",
        "        self.Config = DefaultConfig()\n",
        "        self.max_seq_len = self.Config.max_sequence_length\n",
        "        self.window_size = window_size\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "\n",
        "        count,id_idx,ii,dset,protein_id,seq_length = self.protein_list[index] # Retrieves the information (data) for the given index\n",
        "        window_size = self.window_size # setting window_size to calculate the start and end positions of the sliding window\n",
        "        id_idx = int(id_idx) # index\n",
        "        win_start = ii - window_size # starting point of the sliding window\n",
        "        win_end = ii + window_size # ending point of the sliding window\n",
        "        seq_length = int(seq_length) # sequence length\n",
        "        label_idx = (win_start+win_end)//2 # label index\n",
        "\n",
        "        # Intializing all_seq_features list & seq_len variable\n",
        "        all_seq_features = []\n",
        "        seq_len = 0\n",
        "\n",
        "        # Interating over the sequence data\n",
        "        for idx in self.all_sequences[id_idx][:self.max_seq_len]:\n",
        "            # Converting amino acid indexes into a one hot encoding representation\n",
        "            acid_one_hot = [0 for i in range(20)]\n",
        "            acid_one_hot[idx] = 1\n",
        "            # Storing one hot encoding representations into all_seq_features list\n",
        "            all_seq_features.append(acid_one_hot)\n",
        "            seq_len += 1\n",
        "\n",
        "        # Interating over the sequence data to ensure that the length of all_seq_features_ list is equal to self.max_seq_len\n",
        "        while seq_len<self.max_seq_len:\n",
        "            acid_one_hot = [0 for i in range(20)]\n",
        "            all_seq_features.append(acid_one_hot)\n",
        "            seq_len += 1\n",
        "\n",
        "        # Iterating over the pssm data to ensure that the length of all_pssm_features list is equal to self.max_seq_len\n",
        "        all_pssm_features = self.all_pssm[id_idx][:self.max_seq_len]\n",
        "        seq_len = len(all_pssm_features)\n",
        "        while seq_len<self.max_seq_len:\n",
        "            zero_vector = [0 for i in range(20)]\n",
        "            all_pssm_features.append(zero_vector) # Adding zero vectors if necessary\n",
        "            seq_len += 1\n",
        "\n",
        "        # Iterating over the pssm data to ensure that the length of all_pssm_features list is equal to self.max_seq_len\n",
        "        all_dssp_features = self.all_dssp[id_idx][:self.max_seq_len]\n",
        "        seq_len = len(all_dssp_features)\n",
        "        while seq_len<self.max_seq_len:\n",
        "            zero_vector = [0 for i in range(9)]\n",
        "            all_dssp_features.append(zero_vector) # Adding zero vectors if necessary\n",
        "            seq_len += 1\n",
        "\n",
        "        # Intializing local_features and labels list\n",
        "        local_features = []\n",
        "        labels = []\n",
        "\n",
        "        # Iterating over the sliding window and adding the corresponding features for each postion\n",
        "        # For starting positionless than 0\n",
        "        while win_start<0:\n",
        "            # Intializing data list\n",
        "            data = []\n",
        "            acid_one_hot = [0 for i in range(20)]\n",
        "            data.extend(acid_one_hot) # Adding acid_one_hot\n",
        "\n",
        "            pssm_zero_vector = [0 for i in range(20)]\n",
        "            data.extend(pssm_zero_vector) # Adding ppsm_zero_vector for missing positions\n",
        "\n",
        "            dssp_zero_vector = [0 for i in range(9)]\n",
        "            data.extend(dssp_zero_vector) # Adding dssp_zero_vector for missing positions\n",
        "\n",
        "            # Adding features to the local_features list\n",
        "            local_features.extend(data)\n",
        "            win_start += 1\n",
        "\n",
        "        valid_end = min(win_end,seq_length-1)\n",
        "        # For starting position less than or equal to valid end position\n",
        "        while win_start<=valid_end:\n",
        "            data = []\n",
        "            idx = self.all_sequences[id_idx][win_start]\n",
        "\n",
        "            acid_one_hot = [0 for i in range(20)]\n",
        "            acid_one_hot[idx] = 1\n",
        "            data.extend(acid_one_hot) # Adding acid_one_hot\n",
        "\n",
        "\n",
        "            pssm_val = self.all_pssm[id_idx][win_start]\n",
        "            data.extend(pssm_val) # Adding ppsm_zero_vector\n",
        "\n",
        "            try:\n",
        "                dssp_val = self.all_dssp[id_idx][win_start]\n",
        "            except:\n",
        "                dssp_val = [0 for i in range(9)]\n",
        "            data.extend(dssp_val)  # Adding dssp_zero_vector\n",
        "\n",
        "            # Adding features to the local_features list\n",
        "            local_features.extend(data)\n",
        "            win_start += 1\n",
        "\n",
        "        # For starting position less than or equal to ending position\n",
        "        while win_start<=win_end:\n",
        "            data = []\n",
        "            acid_one_hot = [0 for i in range(20)]\n",
        "            data.extend(acid_one_hot) # Adding acid_one_hot\n",
        "\n",
        "            pssm_zero_vector = [0 for i in range(20)]\n",
        "            data.extend(pssm_zero_vector) # Adding ppsm_zero_vector for missing positions\n",
        "\n",
        "            dssp_zero_vector = [0 for i in range(9)]\n",
        "            data.extend(dssp_zero_vector) # Adding dssp_zero_vector for missing positions\n",
        "\n",
        "            # Adding features to the local_features list\n",
        "            local_features.extend(data)\n",
        "            win_start += 1\n",
        "\n",
        "        # Retrieving label for the central position of the sliding window\n",
        "        label = self.all_label[id_idx][label_idx]\n",
        "        # Converting label list into NumPy array\n",
        "        label = np.array(label,dtype=np.float32)\n",
        "\n",
        "        # Converting all_seq_features list into NumPy array\n",
        "        all_seq_features = np.stack(all_seq_features)\n",
        "        # Resizing all_seq_features NumPy array by adding an extra dimension at the beginning of it\n",
        "        all_seq_features = all_seq_features[np.newaxis,:,:]\n",
        "        # Converting all_pssm_features list into NumPy array\n",
        "        all_pssm_features = np.stack(all_pssm_features)\n",
        "        # Resizing all_pssm_features NumPy array by adding an extra dimension at the beginning of it\n",
        "        all_pssm_features = all_pssm_features[np.newaxis,:,:]\n",
        "        # Converting all_dssp_features list into NumPy array\n",
        "        all_dssp_features = np.stack(all_dssp_features)\n",
        "        # Resizing all_dssp_features NumPy array by adding an extra dimension at the beginning of it\n",
        "        all_dssp_features = all_dssp_features[np.newaxis,:,:]\n",
        "        # Converting local_features list into NumPy array\n",
        "        local_features = np.stack(local_features)\n",
        "\n",
        "\n",
        "        # Returns all_seq_features,all_pssm_features,all_dssp_features,local_features,label NumPy arrays\n",
        "        return all_seq_features,all_pssm_features,all_dssp_features,local_features,label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the total number of proteins in the dataset\n",
        "        return len(self.protein_list)"
      ],
      "metadata": {
        "id": "5LV928snkX4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : The 'dataSet' class in 'data_generator.py' is designed to load and process protein sequence data along with associated features (PSSM, DSSP) and labels. It provides functionality to retrieve specific items from the dataset, preprocess the data, and return it as NumPy arrays\n",
        "2. Why : This custom dataset class can be used in conjunction with PyTorch's data loading utilities to efficiently handle and iterate over protein sequence data during training or evaluation of a machine learning model"
      ],
      "metadata": {
        "id": "Q7h6XO_RmUgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__init__.py (config)"
      ],
      "metadata": {
        "id": "eoTkxFD8klOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import config as Configs"
      ],
      "metadata": {
        "id": "7JpLEyfgkk0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : Imports a module named 'config' from the 'utils' package\n",
        "2. Why : This allows the code to access the classes, functions, or variables defined in that module"
      ],
      "metadata": {
        "id": "AmGBqlZZnh0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "config.py"
      ],
      "metadata": {
        "id": "UVaxGf-FkrZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- encoding:utf8 -*-\n",
        "\n",
        "# Creating DefaulfConfig class that inherits from the object class\n",
        "class DefaultConfig(object):\n",
        "\n",
        "    acid_one_hot = [0 for i in range(20)] # Creating acid_one_hot list and intizlizing it with 20 zeros\n",
        "    acid_idex = {j:i for i,j in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")} # Creating acid_index dictionary and assining each amino acid charater a corresponding index\n",
        "\n",
        "\n",
        "    BASE_PATH = \"../../\" # Creating BASE_PATH string variable\n",
        "    sequence_path = \"{0}/data_cache/sequence_data\".format(BASE_PATH) # Creating a sequence_path string variable which represents the path to the sequence data cache directory by formating the BASE_PATH variable\n",
        "    pssm_path = \"{0}/data_cache/pssm_data\".format(BASE_PATH) # Creating a sequence_path string variable which represents the path to the pssm data cache directory by formating the BASE_PATH variable\n",
        "    dssp_path = \"{0}/data_cache/dssp_data\".format(BASE_PATH)# Creating a sequence_path string variable which represents the path to the dssp data cache directory by formating the BASE_PATH variable\n",
        "\n",
        "    max_sequence_length = 500 # Creating an integer variable max_sequence_length (Maximum length of a proteing sequence) and set it to 500\n",
        "    windows_size = 3 # Creating an integer variable windows_size (Sliding window size) and set it to 3\n",
        "\n",
        "    batch_size = 32 # Creating an integer variabel batch_size (Number of samples) and set it to 32\n",
        "    seq_dim = 20 # Creating an integer variabel seq_dim (Dimensions of the sequnece features) and set it to 20\n",
        "    dssp_dim = 9 # Creating an integer variabel dssp_dim (Dimensions of the dssp features) and set it to 9\n",
        "    pssm_dim = 20 # Creating an integer variabel pssm_dim (Dimensions of the pssm features) and set it to 20\n",
        "\n",
        "    kernels = [13,15,17] # Creating a list kernels (Size of convolutional kernels) which contains the values 13, 15, and 17\n",
        "    dropout =0.2 # Creating a flot variable dropout (Dropout rate) and set it to 0.2\n",
        "    splite_rate = 0.9 # Creating a flot variable splite_rate (Split rate) and set it to 09 (90% data will be used for training and 10% data will be used for testing)"
      ],
      "metadata": {
        "id": "NSk60Vm0kgS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : 'DefaultConfig' class contains various attributes that define configuration settings for a protein sequence analysis task\n",
        "2. Why : These configuration settings provide default values for various parameters and paths used in the protein sequence analysis task. By modifying these attributes, the behavior of the analysis can be adjusted to suit specific requirements or experimental conditions"
      ],
      "metadata": {
        "id": "P6qAL2MunsC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__init__.py (deep_ppi)"
      ],
      "metadata": {
        "id": "TYmo6Tb-kzw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from .deep_ppi import DeepPPI"
      ],
      "metadata": {
        "id": "wg2dYHUCkzOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : Imports 'DeepPPI' module from '.deep_ppi' package\n",
        "2. Why : This allows the code to access the classes, functions, or variables defined in that module"
      ],
      "metadata": {
        "id": "NfXIbIjwns5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BasicModule.py"
      ],
      "metadata": {
        "id": "1ytRDbKJk_a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- encoding:utf-8 -*-\n",
        "\n",
        "import torch as t\n",
        "import time\n",
        "\n",
        "# Creating a class BasicModule that inherits from torch.nn.Module\n",
        "class BasicModule(t.nn.Module):\n",
        "\n",
        "    # Creating a constructor\n",
        "    def __init__(self):\n",
        "        super(BasicModule,self).__init__() # Intialize the parent class torch.nn.Module\n",
        "        self.model_name = str(type(self))\n",
        "\n",
        "    # Creating a load method\n",
        "    def load(self,path):\n",
        "        self.load_state_dict(t.load(path)) # Loads the state dictionary from specified path\n",
        "\n",
        "    # Creating a save method\n",
        "    def save(self,name=None):\n",
        "        if name is None: # Checks if the parameter is not provided to the save method\n",
        "            prefix = \"\" # Intializing an empty string variable premix\n",
        "            name = time.strftime(\"%y%m%d_%H:%M:%S.pth\".format(prefix)) # Generates a timestamp-based name for the saved model file\n",
        "\n",
        "        t.save(self.state_dict(),name) # Saves the state dictionary of the model\n",
        "        return name # Returns the name"
      ],
      "metadata": {
        "id": "jRNa3vrNk-zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : This BasicModule class provides basic functionality for loading and saving models.\n",
        "2. Why : It can be inherited and extended to create custom neural network modules with specific architectures and functionalities"
      ],
      "metadata": {
        "id": "Y5GSgjKintsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "deep_ppi.py"
      ],
      "metadata": {
        "id": "04cMSOBSlEx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- encoding:utf8 -*-\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "\n",
        "import torch as t\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "#from basic_module import BasicModule\n",
        "from models.BasicModule import BasicModule\n",
        "\n",
        "sys.path.append(\"../\") # Appends the parent directory to the system path\n",
        "from utils.config import DefaultConfig\n",
        "configs = DefaultConfig() # Creates an instance of the DefaultConfig class and assings it to the config variable\n",
        "\n",
        "# Creates a ConvsLayer (Convolutional layer module) class that inherits from the BasicModule\n",
        "class ConvsLayer(BasicModule):\n",
        "    # Constructor method\n",
        "    def __init__(self,):\n",
        "\n",
        "        super(ConvsLayer,self).__init__() # Calls the constructor of BasicModule class to intialize it\n",
        "\n",
        "        self.kernels = configs.kernels # Intializes kernels\n",
        "        hidden_channels = configs.cnn_chanel # Intializes hidden_channels\n",
        "        in_channel = 1 # Intializes in_channel (Intial channel)\n",
        "        features_L = configs.max_sequence_length # Intializes features_L (Features length)\n",
        "        seq_dim = configs.seq_dim # Intializes seq_dim (Sequence dimensions)\n",
        "        dssp_dim = configs.dssp_dim # Intializes dssp_dim (DSSP dimensions)\n",
        "        pssm_dim = configs.pssm_dim # Intializes pssm_dim (PSSM dimensions)\n",
        "        W_size = seq_dim + dssp_dim + pssm_dim # Intializes W_size (Window size)\n",
        "\n",
        "        padding1 = (self.kernels[0]-1)//2 # Intializes padding1\n",
        "        padding2 = (self.kernels[1]-1)//2 # Intializes padding2\n",
        "        padding3 = (self.kernels[2]-1)//2 # Intializes padding3\n",
        "        self.conv1 = nn.Sequential() # Adding first Sequential layer\n",
        "        self.conv1.add_module(\"conv1\", # Adding convolutional module nn.Conv2d to the first Sequential layer names conv1\n",
        "            nn.Conv2d(in_channel, hidden_channels,\n",
        "            padding=(padding1,0),\n",
        "            kernel_size=(self.kernels[0],W_size)))\n",
        "        self.conv1.add_module(\"ReLU\",nn.PReLU()) # Adding activation function to the conv1\n",
        "        self.conv1.add_module(\"pooling1\",nn.MaxPool2d(kernel_size=(features_L,1),stride=1)) # Adding max pooling module to the conv1\n",
        "\n",
        "        self.conv2 = nn.Sequential() # Adding second Sequential layer\n",
        "        self.conv2.add_module(\"conv2\", # Adding convolutional module nn.Conv2d to the second Sequential layer names conv2\n",
        "            nn.Conv2d(in_channel, hidden_channels,\n",
        "            padding=(padding2,0),\n",
        "            kernel_size=(self.kernels[1],W_size)))\n",
        "        self.conv2.add_module(\"ReLU\",nn.ReLU()) # Adding activation function to the conv2\n",
        "        self.conv2.add_module(\"pooling2\",nn.MaxPool2d(kernel_size=(features_L,1),stride=1)) # Adding max pooling module to the conv2\n",
        "\n",
        "        self.conv3 = nn.Sequential() # Adding third Sequential layer\n",
        "        self.conv3.add_module(\"conv3\", # Adding convolutional module nn.Conv2d to the third Sequential layer names conv3\n",
        "            nn.Conv2d(in_channel, hidden_channels,\n",
        "            padding=(padding3,0),\n",
        "            kernel_size=(self.kernels[2],W_size)))\n",
        "        self.conv3.add_module(\"ReLU\",nn.ReLU()) # Adding activation function to the conv3\n",
        "        self.conv3.add_module(\"pooling3\",nn.MaxPool2d(kernel_size=(features_L,1),stride=1)) # Adding max pooling module to the conv3\n",
        "\n",
        "    # Creates forward method\n",
        "    def forward(self,x):\n",
        "\n",
        "        features1 = self.conv1(x) # Producing the output features1 by using conv1\n",
        "        features2 = self.conv2(x) # Producing the output features2 by using conv2\n",
        "        features3 = self.conv3(x) # Producing the output features3 by using conv3\n",
        "        features = t.cat((features1,features2,features3),1) # Concatenates the output features from the three sequetial layers and adds another dimension at the end (Now it has total 2 dimensions)\n",
        "        shapes = features.data.shape # Retrieves the shape of the features tensor\n",
        "        features = features.view(shapes[0],shapes[1]*shapes[2]*shapes[3]) # Reshapes the features tensor by flattening the last three (1,2,3) dimensions into a single dimension\n",
        "\n",
        "        return features # Returns the features tensor\n",
        "\n",
        "# Creates a class DeepPPI that inherits from the BasicModule class\n",
        "class DeepPPI(BasicModule):\n",
        "    # Creates a constructor\n",
        "    def __init__(self,class_nums,window_size,ratio=None):\n",
        "        super(DeepPPI,self).__init__() # Calls the constructor of tha parent class BasicModule to intialize it\n",
        "        global configs # Global configs object\n",
        "        configs.kernels = [13, 15, 17] # Sets the kernels 13,15, and 17\n",
        "        self.dropout = configs.dropout = 0.2 # Sets the dropout (Drop rate) 0.2\n",
        "\n",
        "        seq_dim = configs.seq_dim*configs.max_sequence_length # Sets the seq_dim (Sequence dimensions)\n",
        "\n",
        "\n",
        "        self.seq_layers = nn.Sequential() # Sets the seq_layers\n",
        "        self.seq_layers.add_module(\"seq_embedding_layer\", # Adds linear layer called seq_embedding_layer\n",
        "        nn.Linear(seq_dim,seq_dim))\n",
        "        self.seq_layers.add_module(\"seq_embedding_ReLU\", # Adds activation function called seq_embedding_ReLU\n",
        "        nn.ReLU())\n",
        "\n",
        "\n",
        "        seq_dim = configs.seq_dim # Retrieves sequence dimensions from configs object\n",
        "        dssp_dim = configs.dssp_dim # Retrieves dssp dimensions from configs object\n",
        "        pssm_dim = configs.pssm_dim # Retrieves pssm dimensions from configs object\n",
        "        local_dim = (window_size*2+1)*(pssm_dim+dssp_dim+seq_dim) # Calculates local dimensions\n",
        "        if ratio:\n",
        "            configs.cnn_chanel = (local_dim*int(ratio[0]))//(int(ratio[1])*3) # Calculates cnn channel\n",
        "        input_dim = configs.cnn_chanel*3+local_dim # Calculates input dimensions\n",
        "\n",
        "        self.multi_CNN = nn.Sequential() # Creates a sequential container to hold the convolutional layers\n",
        "        self.multi_CNN.add_module(\"layer_convs\",\n",
        "                               ConvsLayer())\n",
        "\n",
        "\n",
        "\n",
        "        self.DNN1 = nn.Sequential() # Creates a sequential container\n",
        "        self.DNN1.add_module(\"DNN_layer1\", # Adds linear layer called DNN_layer1\n",
        "                            nn.Linear(input_dim,1024))\n",
        "        self.DNN1.add_module(\"ReLU1\",\n",
        "                            nn.ReLU()) # Adds activation function called ReLU1\n",
        "        #self.dropout_layer = nn.Dropout(self.dropout)\n",
        "        self.DNN2 = nn.Sequential() # Creates a sequential container\n",
        "        self.DNN2.add_module(\"DNN_layer2\", # Adds linear layer called DNN_layer2\n",
        "                            nn.Linear(1024,256))\n",
        "        self.DNN2.add_module(\"ReLU2\", # Adds activation function called ReLU2\n",
        "                            nn.ReLU())\n",
        "\n",
        "        # outLayer is a sequential container with a single linear layer mapping the input dimension (256) to the number of output classes (class_nums) and a sigmoid activation function\n",
        "        self.outLayer = nn.Sequential(\n",
        "            nn.Linear(256, class_nums),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    # The forward method is defined, which specifies how the input flows through the network during the forward pass\n",
        "    def forward(self,seq,dssp,pssm,local_features):\n",
        "        # Passes input tensors seq,dssp,pssm,local_features through the seq_layers container to apply the sequence embedding layers\n",
        "        shapes = seq.data.shape\n",
        "        features = seq.view(shapes[0],shapes[1]*shapes[2]*shapes[3])\n",
        "        features = self.seq_layers(features)\n",
        "        features = features.view(shapes[0],shapes[1],shapes[2],shapes[3])\n",
        "\n",
        "        features = t.cat((features,dssp,pssm),3)\n",
        "        # Passes concatenated tensor features through the multi_CNN container to apply convolutional layers\n",
        "        features = self.multi_CNN(features)\n",
        "        # Passes concatenated tensor features through the local_features\n",
        "        features = t.cat((features, local_features), 1)\n",
        "        # Passes features tensor through DNN1 container to apply the first set of fully connected layers\n",
        "        features = self.DNN1(features)\n",
        "        #features =self.dropout_layer(features)\n",
        "        # Passes features tensor through DNN2 container to apply the second set of fully connected layers\n",
        "        features = self.DNN2(features)\n",
        "        # Passes final features tensor through the outLayer container, which applies a linear layer followed by a sigmoid activation function\n",
        "        features = self.outLayer(features)\n",
        "\n",
        "        return features # Returns the features tensor"
      ],
      "metadata": {
        "id": "kGvw3NqylFuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : 'ConvsLayer' class contains three sequential layers (conv1, conv2, and conv3), each consisting of a convolutional module, activation function, and max pooling module. In the forward method, it takes an input tensor x and applies the three sequential layers to produce a concatenated output tensor. 'DeepPPI' class contains various sequential layers (seq_layers, multi_CNN, DNN1, and DNN2) to process different types of input features.In the forward method, it takes four input tensors (seq, dssp, pssm, and local_features) and passes them through the model's layers to produce a final output tensor representing the predicted PPI.\n",
        "2. Why : The purpose of these classes is to define the architecture and forward pass of the deep learning model used for PPI prediction. The ConvsLayer class represents a single convolutional layer, while the DeepPPI class combines multiple layers to create a more complex model for PPI prediction."
      ],
      "metadata": {
        "id": "-Q_Mla_dnvoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.py"
      ],
      "metadata": {
        "id": "nx-XrRUOlL6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- encoding:utf8 -*-\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.init import xavier_normal,xavier_normal_\n",
        "from torch import nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "\n",
        "\n",
        "from utils.config import DefaultConfig\n",
        "from models.deep_ppi import DeepPPI\n",
        "from generator import data_generator\n",
        "\n",
        "\n",
        "from evaluation import compute_roc, compute_aupr, compute_mcc, micro_score,acc_score, compute_performance\n",
        "\n",
        "configs = DefaultConfig()\n",
        "THREADHOLD = 0.2\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    Copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def weight_init(m):\n",
        "    if isinstance(m,nn.Conv2d):\n",
        "        xavier_normal_(m.weight.data)\n",
        "    elif isinstance(m,nn.Linear):\n",
        "        xavier_normal_(m.weight.data)\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, optimizer, epoch, all_epochs, print_freq=100):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "\n",
        "    global THREADHOLD\n",
        "    # Model on train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for batch_idx, (seq_data, pssm_data, dssp_data, local_data, label) in enumerate(loader):\n",
        "        # Create vaiables\n",
        "        with torch.no_grad():\n",
        "            if torch.cuda.is_available():\n",
        "                seq_var = torch.autograd.Variable(seq_data.cuda(async=True).float())\n",
        "                pssm_var = torch.autograd.Variable(pssm_data.cuda(async=True).float())\n",
        "                dssp_var = torch.autograd.Variable(dssp_data.cuda(async=True).float())\n",
        "                local_var = torch.autograd.Variable(local_data.cuda(async=True).float())\n",
        "                target_var = torch.autograd.Variable(label.cuda(async=True).float())\n",
        "            else:\n",
        "                seq_var = torch.autograd.Variable(seq_data.float())\n",
        "                pssm_var = torch.autograd.Variable(pssm_data.float())\n",
        "                dssp_var = torch.autograd.Variable(dssp_data.float())\n",
        "                local_var = torch.autograd.Variable(local_data.float())\n",
        "                target_var = torch.autograd.Variable(label.float())\n",
        "\n",
        "        # compute output\n",
        "        output = model(seq_var, dssp_var, pssm_var, local_var)\n",
        "        shapes = output.data.shape\n",
        "        output = output.view(shapes[0]*shapes[1])\n",
        "        loss = torch.nn.functional.binary_cross_entropy(output, target_var).cuda()\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        batch_size = label.size(0)\n",
        "        pred_out = output.ge(THREADHOLD)\n",
        "        MiP, MiR, MiF, PNum, RNum = micro_score(pred_out.data.cpu().numpy(),\n",
        "                                                target_var.data.cpu().numpy())\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print stats\n",
        "        if batch_idx % print_freq == 0:\n",
        "            res = '\\t'.join([\n",
        "                'Epoch: [%d/%d]' % (epoch + 1, all_epochs),\n",
        "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
        "                'f_max:%.6f' % (MiP),\n",
        "                'p_max:%.6f' % (MiR),\n",
        "                'r_max:%.6f' % (MiF),\n",
        "                't_max:%.2f' % (PNum)])\n",
        "            print(res)\n",
        "\n",
        "    return batch_time.avg, losses.avg\n",
        "\n",
        "\n",
        "def eval_epoch(model, loader, print_freq=10, is_test=True):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    error = AverageMeter()\n",
        "\n",
        "    global THREADHOLD\n",
        "    # Model on eval mode\n",
        "    model.eval()\n",
        "\n",
        "    all_trues = []\n",
        "    all_preds = []\n",
        "    all_gos = []\n",
        "    end = time.time()\n",
        "    for batch_idx, (seq_data, pssm_data, dssp_data, local_data, label) in enumerate(loader):\n",
        "\n",
        "        # Create vaiables\n",
        "        with torch.no_grad():\n",
        "            if torch.cuda.is_available():\n",
        "                seq_var = torch.autograd.Variable(seq_data.cuda(async=True).float())\n",
        "                pssm_var = torch.autograd.Variable(pssm_data.cuda(async=True).float())\n",
        "                dssp_var = torch.autograd.Variable(dssp_data.cuda(async=True).float())\n",
        "                local_var = torch.autograd.Variable(local_data.cuda(async=True).float())\n",
        "                target_var = torch.autograd.Variable(label.cuda(async=True).float())\n",
        "            else:\n",
        "                seq_var = torch.autograd.Variable(seq_data.float())\n",
        "                pssm_var = torch.autograd.Variable(pssm_data.float())\n",
        "                dssp_var = torch.autograd.Variable(dssp_data.float())\n",
        "                local_var = torch.autograd.Variable(local_data.float())\n",
        "                target_var = torch.autograd.Variable(label.float())\n",
        "\n",
        "        # compute output\n",
        "        output =  model(seq_var, dssp_var, pssm_var, local_var)\n",
        "        shapes = output.data.shape\n",
        "        output = output.view(shapes[0]*shapes[1])\n",
        "\n",
        "        loss = torch.nn.functional.binary_cross_entropy(output, target_var).cuda()\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        batch_size = label.size(0)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        # print stats\n",
        "        if batch_idx % print_freq == 0:\n",
        "            res = '\\t'.join([\n",
        "                'Test' if is_test else 'Valid',\n",
        "                'Iter: [%d/%d]' % (batch_idx + 1, len(loader)),\n",
        "                'Time %.3f (%.3f)' % (batch_time.val, batch_time.avg),\n",
        "                'Loss %.4f (%.4f)' % (losses.val, losses.avg),\n",
        "            ])\n",
        "            print(res)\n",
        "        all_trues.append(label.numpy())\n",
        "        all_preds.append(output.data.cpu().numpy())\n",
        "\n",
        "    all_trues = np.concatenate(all_trues, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    auc = compute_roc(all_preds, all_trues)\n",
        "    aupr = compute_aupr(all_preds, all_trues)\n",
        "    f_max, p_max, r_max, t_max, predictions_max = compute_performance(all_preds,all_trues)\n",
        "    acc_val = acc_score(predictions_max,all_trues)\n",
        "    mcc = compute_mcc(predictions_max, all_trues)\n",
        "    return batch_time.avg, losses.avg, acc_val, f_max, p_max, r_max, auc, aupr,t_max, mcc\n",
        "\n",
        "\n",
        "def train(class_tag,model, train_data_set, save, n_epochs=3,\n",
        "          batch_size=64, lr=0.001, wd=0.0001, momentum=0.9, seed=None, num=1,\n",
        "          train_file=None):\n",
        "\n",
        "    class_tag = \"all_dset\"\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "    global THREADHOLD\n",
        "    # # split data\n",
        "    with open(train_file,\"rb\") as fp:\n",
        "        train_list = pickle.load(fp)\n",
        "\n",
        "    samples_num =len(train_list)\n",
        "    split_num = int(configs.splite_rate * samples_num)\n",
        "    data_index = train_list\n",
        "    np.random.shuffle(data_index)\n",
        "    train_index = data_index[:split_num]\n",
        "    eval_index = data_index[split_num:]\n",
        "    train_samples = sampler.SubsetRandomSampler(train_index)\n",
        "    eval_samples = sampler.SubsetRandomSampler(eval_index)\n",
        "\n",
        "\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_data_set, batch_size=batch_size,\n",
        "                                               sampler=train_samples, pin_memory=(torch.cuda.is_available()),\n",
        "                                               num_workers=5, drop_last=False)\n",
        "    valid_loader = torch.utils.data.DataLoader(train_data_set, batch_size=batch_size,\n",
        "                                              sampler=eval_samples, pin_memory=(torch.cuda.is_available()),\n",
        "                                               num_workers=5, drop_last=False)\n",
        "    # Model on cuda\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Wrap model for multi-GPUs, if necessary\n",
        "    model_wrapper = model\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(model_wrapper.parameters(), lr=0.001)\n",
        "\n",
        "    # Start log\n",
        "    with open(os.path.join(save, 'DeepPPI_results.csv'), 'w') as f:\n",
        "        f.write('epoch,loss,acc,F_value, precision,recall,auc,aupr,mcc,threadhold\\n')\n",
        "\n",
        "        # Train model\n",
        "        best_F = 0\n",
        "        threadhold = 0\n",
        "        count = 0\n",
        "        for epoch in range(n_epochs):\n",
        "            _, train_loss = train_epoch(\n",
        "                model=model_wrapper,\n",
        "                loader=train_loader,\n",
        "                optimizer=optimizer,\n",
        "                epoch=epoch,\n",
        "                all_epochs=n_epochs,\n",
        "            )\n",
        "            _, valid_loss, acc, f_max, p_max, r_max, auc, aupr,t_max,mcc= eval_epoch(\n",
        "                model=model_wrapper,\n",
        "                loader=valid_loader,\n",
        "                is_test=(not valid_loader)\n",
        "            )\n",
        "\n",
        "            print(\n",
        "            'epoch:%03d,valid_loss:%0.5f\\nacc:%0.6f,F_value:%0.6f, precision:%0.6f,recall:%0.6f,auc:%0.6f,aupr:%0.6f,mcc:%0.6f,threadhold:%0.6f\\n' % (\n",
        "                (epoch + 1), valid_loss, acc, f_max, p_max, r_max,auc, aupr,mcc,t_max))\n",
        "            if f_max > best_F:\n",
        "                count = 0\n",
        "                best_F = f_max\n",
        "                THREADHOLD = t_max\n",
        "                print(\"new best F_value:{0}(threadhold:{1})\".format(f_max, THREADHOLD))\n",
        "                torch.save(model.state_dict(), os.path.join(save, 'DeepPPI_model.dat'))\n",
        "            else:\n",
        "                count += 1\n",
        "                if count>=5:\n",
        "                    return None\n",
        "            # Log results\n",
        "            f.write('%03d,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f\\n' % (\n",
        "                (epoch + 1), valid_loss, acc, f_max, p_max, r_max, auc, aupr,mcc,t_max))\n",
        "\n",
        "\n",
        "\n",
        "def demo(train_data,save=None, train_num = 1,\n",
        "    ratio=None,window_size=3,splite_rate = 0.1, efficient=True,\n",
        "              epochs=10, seed=None,pretrained_result=None):\n",
        "\n",
        "    train_sequences_file = ['data_cache/{0}_sequence_data.pkl'.format(key) for key in train_data]\n",
        "    train_dssp_file = ['data_cache/{0}_dssp_data.pkl'.format(key) for key in train_data]\n",
        "    train_pssm_file = ['data_cache/{0}_pssm_data.pkl'.format(key) for key in train_data]\n",
        "    train_label_file = ['data_cache/{0}_label.pkl'.format(key) for key in train_data]\n",
        "    all_list_file = 'data_cache/all_dset_list.pkl'\n",
        "    train_list_file = 'data_cache/training_list.pkl'\n",
        "\n",
        "\n",
        "    #parameters\n",
        "    batch_size = configs.batch_size\n",
        "\n",
        "    # Datasets\n",
        "    train_dataSet = data_generator.dataSet(window_size, train_sequences_file, train_pssm_file, train_dssp_file, train_label_file,\n",
        "                                             all_list_file)\n",
        "    # Models\n",
        "\n",
        "    class_nums = 1\n",
        "    model = DeepPPI(class_nums,window_size,ratio)\n",
        "    model.apply(weight_init)\n",
        "\n",
        "    # Train the model\n",
        "    train(train_data,model=model, train_data_set=train_dataSet, save=save,\n",
        "          n_epochs=epochs, batch_size=batch_size, seed=seed,num=train_num,\n",
        "          train_file=train_list_file)\n",
        "    print('Done!')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    ratio_list = (2,1)  #glboal:local\n",
        "    path_dir = \"./checkpoints/deep_ppi_saved_models\"\n",
        "    train_data = [\"dset186\",\"dset164\",\"dset72\"]\n",
        "    if not os.path.exists(path_dir):\n",
        "        os.makedirs(path_dir)\n",
        "\n",
        "    for ii in range(1,5):\n",
        "        demo(train_data,path_dir,ii,ratio_list)\n"
      ],
      "metadata": {
        "id": "tbscWJ0OlLdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What & Why: It trains the deep learning model for protein-protein interaction prediction using different training datasets (train_data). It performs multiple training runs (train_num) with different ratios (ratio_list) of global to local features. The trained models are saved in the specified directory (path_dir)."
      ],
      "metadata": {
        "id": "YTMUsfKXnxpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict.py"
      ],
      "metadata": {
        "id": "xh0vNbPSlRfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- encoding:utf8 -*-\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.nn.init import xavier_normal,xavier_normal_\n",
        "from torch import nn\n",
        "import torch.utils.data.sampler as sampler\n",
        "\n",
        "\n",
        "from utils.config import DefaultConfig\n",
        "from models.deep_ppi import DeepPPI\n",
        "from generator import data_generator\n",
        "\n",
        "\n",
        "from evaluation import compute_roc, compute_aupr, compute_mcc, micro_score,acc_score, compute_performance\n",
        "\n",
        "# Creates an instance of the DefaultConfig class and set a threshold value to 0.2\n",
        "configs = DefaultConfig()\n",
        "THREADHOLD = 0.2\n",
        "\n",
        "# Defines a class named AverageMeter that is used for computing and storing the average and current values. It includes methods for resetting the values and updating them\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    Copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# Defines a function named weight_init that initializes the weights of the model using the Xavier normal initialization method. It takes a model m as input and initializes the weights of convolutional and linear layers\n",
        "def weight_init(m):\n",
        "    if isinstance(m,nn.Conv2d):\n",
        "        xavier_normal_(m.weight.data)\n",
        "    elif isinstance(m,nn.Linear):\n",
        "        xavier_normal_(m.weight.data)\n",
        "\n",
        "# Defines a function named test that performs testing on the trained model\n",
        "def test(model, loader,path_dir,pre_num=1):\n",
        "\n",
        "    # Model on eval mode\n",
        "    model.eval() # The model is set to evaluation mode\n",
        "    length = len(loader)\n",
        "    result = []\n",
        "    all_trues = []\n",
        "\n",
        "    for batch_idx, (seq_data, pssm_data, dssp_data, local_data, label) in enumerate(loader):\n",
        "\n",
        "        # Create vaiables\n",
        "        with torch.no_grad():\n",
        "            if torch.cuda.is_available():\n",
        "                seq_var = torch.autograd.Variable(seq_data.cuda(async=True).float())\n",
        "                pssm_var = torch.autograd.Variable(pssm_data.cuda(async=True).float())\n",
        "                dssp_var = torch.autograd.Variable(dssp_data.cuda(async=True).float())\n",
        "                local_var = torch.autograd.Variable(local_data.cuda(async=True).float())\n",
        "                target_var = torch.autograd.Variable(label.cuda(async=True).float())\n",
        "            else:\n",
        "                seq_var = torch.autograd.Variable(seq_data.float())\n",
        "                pssm_var = torch.autograd.Variable(pssm_data.float())\n",
        "                dssp_var = torch.autograd.Variable(dssp_data.float())\n",
        "                local_var = torch.autograd.Variable(local_data.float())\n",
        "                target_var = torch.autograd.Variable(label.float())\n",
        "\n",
        "        # compute output\n",
        "        output =  model(seq_var, dssp_var, pssm_var, local_var)\n",
        "        shapes = output.data.shape\n",
        "        output = output.view(shapes[0]*shapes[1])\n",
        "        result.append(output.data.cpu().numpy())\n",
        "        all_trues.append(label.numpy())\n",
        "\n",
        "\n",
        "    #Caculates the predictions and true labels\n",
        "    all_trues = np.concatenate(all_trues, axis=0)\n",
        "    all_preds = np.concatenate(result, axis=0)\n",
        "\n",
        "    # Calculates evaluation metrics such as area under the ROC curve (AUC), area under the precision-recall curve (AUPR), F1 score, accuracy, and Matthews correlation coefficient (MCC)\n",
        "    auc = compute_roc(all_preds, all_trues)\n",
        "    aupr = compute_aupr(all_preds, all_trues)\n",
        "    f_max, p_max, r_max, t_max, predictions_max = compute_performance(all_preds,all_trues)\n",
        "    acc = acc_score(predictions_max,all_trues)\n",
        "    mcc = compute_mcc(predictions_max, all_trues)\n",
        "\n",
        "    print(\n",
        "        'acc:%0.6f,F_value:%0.6f, precision:%0.6f,recall:%0.6f,auc:%0.6f,aupr:%0.6f,mcc:%0.6f,threadhold:%0.6f\\n' % (\n",
        "        acc, f_max, p_max, r_max,auc, aupr,mcc,t_max))\n",
        "\n",
        "\n",
        "\n",
        "    predict_result = {}\n",
        "    predict_result[\"pred\"] = all_preds\n",
        "    predict_result[\"label\"] = all_trues\n",
        "    result_file = \"{0}/test_predict.pkl\".format(path_dir)\n",
        "    with open(result_file,\"wb\") as fp:\n",
        "        pickle.dump(predict_result,fp)\n",
        "\n",
        "# Defines a function named predict that performs prediction using a trained model\n",
        "def predict(model_file,test_data,window_size,path_dir,ratio):\n",
        "    test_sequences_file = ['data_cache/{0}_sequence_data.pkl'.format(key) for key in test_data]\n",
        "    test_dssp_file = ['data_cache/{0}_dssp_data.pkl'.format(key) for key in test_data]\n",
        "    test_pssm_file = ['data_cache/{0}_pssm_data.pkl'.format(key) for key in test_data]\n",
        "    test_label_file = ['data_cache/{0}_label.pkl'.format(key) for key in test_data]\n",
        "    all_list_file = 'data_cache/all_dset_list.pkl'\n",
        "    test_list_file = 'data_cache/testing_list.pkl'\n",
        "    # parameters\n",
        "    batch_size = configs.batch_size\n",
        "\n",
        "    print(test_list_file)\n",
        "    #parameters\n",
        "    batch_size = configs.batch_size\n",
        "\n",
        "    # Datasets\n",
        "    test_dataSet = data_generator.dataSet(window_size, test_sequences_file, test_pssm_file, test_dssp_file, test_label_file,\n",
        "                                             all_list_file) # Loads the test data  from files\n",
        "    # Models\n",
        "    with open(test_list_file,\"rb\") as fp:\n",
        "        test_list = pickle.load(fp) # Creates a data loader for the test data\n",
        "\n",
        "    test_samples = sampler.SubsetRandomSampler(test_list)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataSet, batch_size=batch_size,\n",
        "                                              sampler=test_samples, pin_memory=(torch.cuda.is_available()),\n",
        "                                               num_workers=5, drop_last=False)\n",
        "\n",
        "    # Models\n",
        "    class_nums = 1\n",
        "    model = DeepPPI(class_nums,window_size,ratio) # Creates An instance of the DeepPPI model and loaded with the trained weights from the model file\n",
        "    model.load_state_dict(torch.load(model_file))\n",
        "    model = model.cuda()\n",
        "    test(model, test_loader,path_dir) # Calls test function to perform testing and evaluate the model's performance on the test data\n",
        "\n",
        "    print('Done!')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "\n",
        "    ratio_list = (2,1) #glboal:local\n",
        "    window_size = 3\n",
        "    path_dir = \"./checkpoints/deep_ppi_saved_models\"\n",
        "\n",
        "    datas = [\"dset186\",\"dset164\",\"dset72\"]\n",
        "    if not os.path.exists(path_dir):\n",
        "        os.makedirs(path_dir)\n",
        "\n",
        "    model_file = \"{0}/DeepPPI_model.dat\".format(path_dir)\n",
        "    predict(model_file,datas,window_size,path_dir,ratio_list)"
      ],
      "metadata": {
        "id": "6Mj2zm7elQ-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What & Why : The code loads a trained DeepPPI model, performs testing on the test data, and evaluates the model's performance by calculating various evaluation metrics. It also saves the predicted results for further analysis."
      ],
      "metadata": {
        "id": "ZyeTUE9LnyvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluation.py"
      ],
      "metadata": {
        "id": "zS6LTutWlYz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-*- encoding:utf8 -*-\n",
        "#!/usr/bin/env python\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import pickle\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, matthews_corrcoef, precision_recall_curve,accuracy_score\n",
        "\n",
        "# ROC - Receiver Operating Characteristic\n",
        "# AUC - Area Under the Curve\n",
        "\n",
        "def compute_roc(preds, labels):\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    fpr, tpr, _ = roc_curve(labels.flatten(), preds.flatten())\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "def compute_aupr(preds, labels):\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    p, r, _ = precision_recall_curve(labels.flatten(), preds.flatten())\n",
        "    aupr = auc(r, p)\n",
        "    return aupr\n",
        "\n",
        "\n",
        "def compute_mcc(preds, labels, threshold=0.5):\n",
        "    preds = preds.astype(np.float64)\n",
        "    labels = labels.astype(np.float64)\n",
        "    # Compute ROC curve and ROC area for each class\n",
        "    mcc = matthews_corrcoef(labels.flatten(), preds.flatten())\n",
        "    return mcc\n",
        "\n",
        "\n",
        "def compute_performance(preds, labels):\n",
        "\n",
        "    predictions_max = None # Sets predictions_max to None\n",
        "    # Initializes f_max, p_max, r_max, t_max\n",
        "    f_max = 0\n",
        "    p_max = 0\n",
        "    r_max = 0\n",
        "    t_max = 0\n",
        "    for t in range(1, 100): # Iterates over threshold values from 0.01 to 0.99\n",
        "        threshold = t / 100.0\n",
        "        predictions = (preds > threshold).astype(np.int32) # Calulates predictions\n",
        "        p = 0.0\n",
        "        r = 0.0\n",
        "        total = 0\n",
        "        p_total = 0\n",
        "\n",
        "        tp = np.sum(predictions * labels) # Calulates true positives (tp)\n",
        "        fp = np.sum(predictions) - tp # Calculates false positives (fp)\n",
        "        fn = np.sum(labels) - tp # Calculates false negatives (fn)\n",
        "\n",
        "        if tp == 0 and fp == 0 and fn == 0: # Checks if tp,fp and fn are equal to 0\n",
        "            continue # If so, it continues to the next iteration of the loop\n",
        "        total += 1 # Otherwise, it increments the total by 1\n",
        "        if tp != 0: # If tp is non-zero\n",
        "            p_total += 1 # It increments p_total by 1\n",
        "            precision = tp / (1.0 * (tp + fp)) # Calculates precision\n",
        "            recall = tp / (1.0 * (tp + fn)) # Calculates recall\n",
        "            p += precision # Adds precision to p\n",
        "            r += recall # Adds recall to r\n",
        "\n",
        "        if total > 0 and p_total > 0: # if total and p_total is greater than zero\n",
        "            r /= total # Computes recall (r)\n",
        "            p /= p_total # Computes precision (p)\n",
        "            if p + r > 0: # If precision (p) + recall (r) is greater than zero\n",
        "                f = 2 * p * r / (p + r) # Calculates F1 Score (f) : 2 * precision * recall / (precision + recall)\n",
        "                if f_max < f: # If f_max is less than f\n",
        "                    f_max = f # Updates the f_max\n",
        "                    p_max = p # Updates the p_max\n",
        "                    r_max = r # Updates the r_max\n",
        "                    t_max = threshold # Updates the t_max\n",
        "                    predictions_max = predictions # Updates the predictions_max\n",
        "\n",
        "    return f_max, p_max, r_max, t_max, predictions_max # Returns f_max, p_max, r_max, t_max, predictions_max\n",
        "\n",
        "# Calulates various evaluation metrics for a binary classification task\n",
        "def micro_score(output, label):\n",
        "    N = len(output) # Calculates the total number of instances (N)\n",
        "    total_P = np.sum(output) # Calculates sum of positiove predictions (total_P)\n",
        "    total_R = np.sum(label) # Calculates sum of true labels (total_R)\n",
        "    TP = float(np.sum(output * label)) # Calculates trues positives (TP)\n",
        "    MiP = TP / max(total_P, 1e-12) # Calculates micro-precision (MiP)\n",
        "    MiR = TP / max(total_R, 1e-12) # Calculates micro-recall (MiR)\n",
        "    if TP==0: # If true positives equals to 0\n",
        "        MiF = 0 # Sets micro-F1 (MiF) Score to 0\n",
        "    else: # Else\n",
        "        MiF = 2 * MiP * MiR / (MiP + MiR) # Calculates micro-F1 (MiF) using the following formula : 2 * MiP * MiR / (MiP + MiR)\n",
        "    return MiP, MiR, MiF, total_P / N, total_R / N # Returns MiP, MiR, MiF, total_P / N, total_R / N\n",
        "\n",
        "def acc_score(output,label):\n",
        "    acc = accuracy_score(label.flatten(), output.flatten()) # Calculates the accuracy\n",
        "    return acc # Return the accuracy\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass"
      ],
      "metadata": {
        "id": "E3ypGMNjlYpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What : The code provides utility functions for evaluating the performance of binary classification models by computing metrics such as ROC-AUC, precision-recall AUC, Matthews correlation coefficient, F1 score, micro-precision, micro-recall, micro-F1 score, and accuracy.\n",
        "2. Why : These metrics are commonly used to assess the quality of classification models."
      ],
      "metadata": {
        "id": "GT8eKqFFnzfT"
      }
    }
  ]
}